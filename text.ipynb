{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import wave\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import LSTM, Input, Flatten, Merge, Embedding, Convolution1D,Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "from features import *\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n",
    "data_path = code_path + \"/../data/sessions/\"\n",
    "sessions = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']\n",
    "framerate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(data_path + '/../'+'data_collected.pickle', 'rb') as handle:\n",
    "    data2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for ses_mod in data2:\n",
    "    text.append(ses_mod['transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "token_tr_X = tokenizer.texts_to_sequences(text)\n",
    "x_train_text = []\n",
    "\n",
    "x_train_text = sequence.pad_sequences(token_tr_X, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2736 unique tokens\n",
      "/home/samarth/emotion_recognition-master/code/../data/sessions/../glove.42B.300d.txt\n",
      "G Word embeddings: 1917494\n",
      "G Null word embeddings: 90\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "file_loc = data_path + '../glove.42B.300d.txt'\n",
    "\n",
    "print (file_loc)\n",
    "\n",
    "gembeddings_index = {}\n",
    "with codecs.open(file_loc, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        gembedding = np.asarray(values[1:], dtype='float32')\n",
    "        gembeddings_index[word] = gembedding\n",
    "#\n",
    "f.close()\n",
    "print('G Word embeddings:', len(gembeddings_index))\n",
    "\n",
    "nb_words = len(word_index) +1\n",
    "g_word_embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    gembedding_vector = gembeddings_index.get(word)\n",
    "    if gembedding_vector is not None:\n",
    "        g_word_embedding_matrix[i] = gembedding_vector\n",
    "        \n",
    "print('G Null word embeddings: %d' % np.sum(np.sum(g_word_embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4936, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=[]\n",
    "for ses_mod in data2:\n",
    "    Y.append(ses_mod['emotion'])\n",
    "    \n",
    "Y = label_binarize(Y,emotions_used)\n",
    "\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 300)          821100    \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 500, 256)          230656    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 500, 128)          98432     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 500, 64)           24640     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 500, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               4096256   \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 5,278,288\n",
      "Trainable params: 5,278,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model1 Built\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(256, 3, padding=\"same\")`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(128, 3, padding=\"same\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, padding=\"same\")`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(32, 3, padding=\"same\")`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(2737, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Embedding(nb_words,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights = [g_word_embedding_matrix],\n",
    "                    input_length = MAX_SEQUENCE_LENGTH,\n",
    "                    trainable = True))\n",
    "model.add(Convolution1D(256, 3, border_mode='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(128, 3, border_mode='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(64, 3, border_mode='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution1D(32, 3, border_mode='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam' ,metrics=['acc'])\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "#model.compile()\n",
    "model.summary()\n",
    "\n",
    "print(\"Model1 Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/125\n",
      "3948/3948 [==============================] - 5s - loss: 1.3701 - acc: 0.3379 - val_loss: 1.3694 - val_acc: 0.3694\n",
      "Epoch 2/125\n",
      "3948/3948 [==============================] - 4s - loss: 1.3621 - acc: 0.3432 - val_loss: 1.3285 - val_acc: 0.3887\n",
      "Epoch 3/125\n",
      "3948/3948 [==============================] - 4s - loss: 1.3026 - acc: 0.3744 - val_loss: 1.2241 - val_acc: 0.4494\n",
      "Epoch 4/125\n",
      "3948/3948 [==============================] - 4s - loss: 1.2280 - acc: 0.4240 - val_loss: 1.2129 - val_acc: 0.4514\n",
      "Epoch 5/125\n",
      "3948/3948 [==============================] - 4s - loss: 1.1324 - acc: 0.4828 - val_loss: 1.2270 - val_acc: 0.4555\n",
      "Epoch 6/125\n",
      "3948/3948 [==============================] - 4s - loss: 1.0153 - acc: 0.5415 - val_loss: 1.2696 - val_acc: 0.4727\n",
      "Epoch 7/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.9095 - acc: 0.6125 - val_loss: 1.3892 - val_acc: 0.4828\n",
      "Epoch 8/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.8235 - acc: 0.6525 - val_loss: 1.3975 - val_acc: 0.5061\n",
      "Epoch 9/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.7392 - acc: 0.7009 - val_loss: 1.5488 - val_acc: 0.5172\n",
      "Epoch 10/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.6659 - acc: 0.7323 - val_loss: 1.5916 - val_acc: 0.5132\n",
      "Epoch 11/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.6302 - acc: 0.7581 - val_loss: 1.6072 - val_acc: 0.4929\n",
      "Epoch 12/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.5734 - acc: 0.7794 - val_loss: 1.7861 - val_acc: 0.5294\n",
      "Epoch 13/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.5123 - acc: 0.8014 - val_loss: 2.1560 - val_acc: 0.5182\n",
      "Epoch 14/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.4806 - acc: 0.8131 - val_loss: 1.8416 - val_acc: 0.5638\n",
      "Epoch 15/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.4376 - acc: 0.8346 - val_loss: 1.8978 - val_acc: 0.5455\n",
      "Epoch 16/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.3906 - acc: 0.8460 - val_loss: 2.1863 - val_acc: 0.5577\n",
      "Epoch 17/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.3702 - acc: 0.8592 - val_loss: 2.0638 - val_acc: 0.5496\n",
      "Epoch 18/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.3638 - acc: 0.8582 - val_loss: 2.1222 - val_acc: 0.5597\n",
      "Epoch 19/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.3561 - acc: 0.8675 - val_loss: 2.2230 - val_acc: 0.5628\n",
      "Epoch 20/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.3174 - acc: 0.8744 - val_loss: 2.2503 - val_acc: 0.5557\n",
      "Epoch 21/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.3045 - acc: 0.8812 - val_loss: 2.4668 - val_acc: 0.5476\n",
      "Epoch 22/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.3031 - acc: 0.8868 - val_loss: 2.2243 - val_acc: 0.5840\n",
      "Epoch 23/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2724 - acc: 0.8898 - val_loss: 2.4054 - val_acc: 0.5607\n",
      "Epoch 24/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2791 - acc: 0.8926 - val_loss: 2.4979 - val_acc: 0.5698\n",
      "Epoch 25/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2719 - acc: 0.8961 - val_loss: 2.3057 - val_acc: 0.5719\n",
      "Epoch 26/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2442 - acc: 0.9065 - val_loss: 2.4268 - val_acc: 0.5759\n",
      "Epoch 27/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2349 - acc: 0.9083 - val_loss: 2.4706 - val_acc: 0.5789\n",
      "Epoch 28/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2321 - acc: 0.9134 - val_loss: 2.5028 - val_acc: 0.5789\n",
      "Epoch 29/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2502 - acc: 0.9055 - val_loss: 2.6596 - val_acc: 0.5729\n",
      "Epoch 30/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2418 - acc: 0.9050 - val_loss: 2.5214 - val_acc: 0.5648\n",
      "Epoch 31/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2143 - acc: 0.9146 - val_loss: 2.6680 - val_acc: 0.6002\n",
      "Epoch 32/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2132 - acc: 0.9157 - val_loss: 2.7789 - val_acc: 0.5860\n",
      "Epoch 33/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2070 - acc: 0.9222 - val_loss: 2.6716 - val_acc: 0.5810\n",
      "Epoch 34/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2138 - acc: 0.9212 - val_loss: 2.7022 - val_acc: 0.5688\n",
      "Epoch 35/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1941 - acc: 0.9195 - val_loss: 2.7775 - val_acc: 0.5800\n",
      "Epoch 36/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1830 - acc: 0.9240 - val_loss: 2.6369 - val_acc: 0.5779\n",
      "Epoch 37/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1796 - acc: 0.9248 - val_loss: 2.9725 - val_acc: 0.5901\n",
      "Epoch 38/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1839 - acc: 0.9240 - val_loss: 2.9468 - val_acc: 0.5719\n",
      "Epoch 39/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1900 - acc: 0.9271 - val_loss: 2.8805 - val_acc: 0.5628\n",
      "Epoch 40/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.2054 - acc: 0.9207 - val_loss: 2.7360 - val_acc: 0.6012\n",
      "Epoch 41/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1927 - acc: 0.9260 - val_loss: 2.6015 - val_acc: 0.5901\n",
      "Epoch 42/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1776 - acc: 0.9291 - val_loss: 2.6803 - val_acc: 0.5891\n",
      "Epoch 43/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1743 - acc: 0.9260 - val_loss: 2.7870 - val_acc: 0.5941\n",
      "Epoch 44/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1638 - acc: 0.9334 - val_loss: 3.0396 - val_acc: 0.5779\n",
      "Epoch 45/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1563 - acc: 0.9306 - val_loss: 3.0719 - val_acc: 0.5779\n",
      "Epoch 46/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1661 - acc: 0.9324 - val_loss: 2.7690 - val_acc: 0.5698\n",
      "Epoch 47/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1805 - acc: 0.9281 - val_loss: 2.9738 - val_acc: 0.5800\n",
      "Epoch 48/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1674 - acc: 0.9301 - val_loss: 3.2340 - val_acc: 0.5800\n",
      "Epoch 49/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1837 - acc: 0.9293 - val_loss: 2.8949 - val_acc: 0.5729\n",
      "Epoch 50/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1756 - acc: 0.9309 - val_loss: 2.9989 - val_acc: 0.5931\n",
      "Epoch 51/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1757 - acc: 0.9352 - val_loss: 2.8558 - val_acc: 0.5881\n",
      "Epoch 52/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1828 - acc: 0.9230 - val_loss: 2.6930 - val_acc: 0.5860\n",
      "Epoch 53/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1802 - acc: 0.9271 - val_loss: 2.7780 - val_acc: 0.5749\n",
      "Epoch 54/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1669 - acc: 0.9286 - val_loss: 2.7875 - val_acc: 0.6002\n",
      "Epoch 55/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1572 - acc: 0.9336 - val_loss: 2.8922 - val_acc: 0.5972\n",
      "Epoch 56/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1422 - acc: 0.9407 - val_loss: 3.1669 - val_acc: 0.5992\n",
      "Epoch 57/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1488 - acc: 0.9364 - val_loss: 2.9800 - val_acc: 0.5881\n",
      "Epoch 58/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1430 - acc: 0.9384 - val_loss: 3.1372 - val_acc: 0.5891\n",
      "Epoch 59/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1391 - acc: 0.9390 - val_loss: 3.2319 - val_acc: 0.6255\n",
      "Epoch 60/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1472 - acc: 0.9354 - val_loss: 3.1293 - val_acc: 0.6083\n",
      "Epoch 61/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1560 - acc: 0.9364 - val_loss: 3.1613 - val_acc: 0.5901\n",
      "Epoch 62/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1485 - acc: 0.9309 - val_loss: 3.1222 - val_acc: 0.5982\n",
      "Epoch 63/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1422 - acc: 0.9349 - val_loss: 3.1659 - val_acc: 0.5962\n",
      "Epoch 64/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1519 - acc: 0.9387 - val_loss: 3.0230 - val_acc: 0.6063\n",
      "Epoch 65/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1503 - acc: 0.9334 - val_loss: 2.9990 - val_acc: 0.5901\n",
      "Epoch 66/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1464 - acc: 0.9347 - val_loss: 3.3925 - val_acc: 0.5789\n",
      "Epoch 67/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1397 - acc: 0.9377 - val_loss: 3.0590 - val_acc: 0.6073\n",
      "Epoch 68/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1643 - acc: 0.9336 - val_loss: 3.1661 - val_acc: 0.5800\n",
      "Epoch 69/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1618 - acc: 0.9298 - val_loss: 3.1119 - val_acc: 0.5729\n",
      "Epoch 70/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1577 - acc: 0.9329 - val_loss: 3.0898 - val_acc: 0.6063\n",
      "Epoch 71/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1402 - acc: 0.9415 - val_loss: 3.1687 - val_acc: 0.5860\n",
      "Epoch 72/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1400 - acc: 0.9415 - val_loss: 3.0215 - val_acc: 0.5941\n",
      "Epoch 73/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1360 - acc: 0.9384 - val_loss: 3.2455 - val_acc: 0.5992\n",
      "Epoch 74/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1404 - acc: 0.9407 - val_loss: 3.0679 - val_acc: 0.5901\n",
      "Epoch 75/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1306 - acc: 0.9384 - val_loss: 3.3643 - val_acc: 0.5881\n",
      "Epoch 76/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1347 - acc: 0.9395 - val_loss: 3.3098 - val_acc: 0.6053\n",
      "Epoch 77/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1287 - acc: 0.9405 - val_loss: 3.3481 - val_acc: 0.5840\n",
      "Epoch 78/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1345 - acc: 0.9405 - val_loss: 3.2314 - val_acc: 0.6144\n",
      "Epoch 79/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1422 - acc: 0.9402 - val_loss: 3.1501 - val_acc: 0.5921\n",
      "Epoch 80/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1368 - acc: 0.9390 - val_loss: 3.2473 - val_acc: 0.5931\n",
      "Epoch 81/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1405 - acc: 0.9369 - val_loss: 3.2168 - val_acc: 0.6022\n",
      "Epoch 82/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1369 - acc: 0.9382 - val_loss: 3.1687 - val_acc: 0.5972\n",
      "Epoch 83/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1274 - acc: 0.9400 - val_loss: 3.3429 - val_acc: 0.5992\n",
      "Epoch 84/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1267 - acc: 0.9433 - val_loss: 3.3387 - val_acc: 0.6113\n",
      "Epoch 85/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1371 - acc: 0.9390 - val_loss: 3.3633 - val_acc: 0.6134\n",
      "Epoch 86/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1384 - acc: 0.9397 - val_loss: 3.1476 - val_acc: 0.6113\n",
      "Epoch 87/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1386 - acc: 0.9369 - val_loss: 3.3757 - val_acc: 0.5951\n",
      "Epoch 88/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1412 - acc: 0.9382 - val_loss: 3.3308 - val_acc: 0.6184\n",
      "Epoch 89/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1380 - acc: 0.9392 - val_loss: 3.4021 - val_acc: 0.6073\n",
      "Epoch 90/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1393 - acc: 0.9407 - val_loss: 3.2911 - val_acc: 0.6063\n",
      "Epoch 91/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1424 - acc: 0.9374 - val_loss: 3.2747 - val_acc: 0.5911\n",
      "Epoch 92/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1335 - acc: 0.9400 - val_loss: 3.4193 - val_acc: 0.5972\n",
      "Epoch 93/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1303 - acc: 0.9435 - val_loss: 3.4309 - val_acc: 0.6093\n",
      "Epoch 94/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1277 - acc: 0.9407 - val_loss: 3.4531 - val_acc: 0.5840\n",
      "Epoch 95/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1372 - acc: 0.9392 - val_loss: 3.3910 - val_acc: 0.6063\n",
      "Epoch 96/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1348 - acc: 0.9397 - val_loss: 3.3707 - val_acc: 0.5830\n",
      "Epoch 97/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1246 - acc: 0.9420 - val_loss: 3.4397 - val_acc: 0.6002\n",
      "Epoch 98/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1440 - acc: 0.9344 - val_loss: 3.3093 - val_acc: 0.6113\n",
      "Epoch 99/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1271 - acc: 0.9443 - val_loss: 3.3029 - val_acc: 0.6063\n",
      "Epoch 100/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1212 - acc: 0.9438 - val_loss: 3.4782 - val_acc: 0.6123\n",
      "Epoch 101/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1296 - acc: 0.9390 - val_loss: 3.4396 - val_acc: 0.6134\n",
      "Epoch 102/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1294 - acc: 0.9420 - val_loss: 3.3841 - val_acc: 0.6002\n",
      "Epoch 103/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1235 - acc: 0.9445 - val_loss: 3.3612 - val_acc: 0.5962\n",
      "Epoch 104/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1289 - acc: 0.9400 - val_loss: 3.3264 - val_acc: 0.6053\n",
      "Epoch 105/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1248 - acc: 0.9428 - val_loss: 3.4457 - val_acc: 0.5881\n",
      "Epoch 106/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1288 - acc: 0.9395 - val_loss: 3.2368 - val_acc: 0.6032\n",
      "Epoch 107/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1570 - acc: 0.9344 - val_loss: 2.9965 - val_acc: 0.6002\n",
      "Epoch 108/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1474 - acc: 0.9379 - val_loss: 3.4624 - val_acc: 0.5840\n",
      "Epoch 109/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1517 - acc: 0.9352 - val_loss: 3.1381 - val_acc: 0.5820\n",
      "Epoch 110/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1510 - acc: 0.9359 - val_loss: 3.1798 - val_acc: 0.5860\n",
      "Epoch 111/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1508 - acc: 0.9374 - val_loss: 3.3006 - val_acc: 0.6073\n",
      "Epoch 112/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1312 - acc: 0.9415 - val_loss: 3.3563 - val_acc: 0.6053\n",
      "Epoch 113/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1312 - acc: 0.9417 - val_loss: 3.2275 - val_acc: 0.5951\n",
      "Epoch 114/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1333 - acc: 0.9390 - val_loss: 3.1419 - val_acc: 0.6144\n",
      "Epoch 115/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1332 - acc: 0.9412 - val_loss: 3.1386 - val_acc: 0.5951\n",
      "Epoch 116/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1239 - acc: 0.9410 - val_loss: 3.2606 - val_acc: 0.5982\n",
      "Epoch 117/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1226 - acc: 0.9410 - val_loss: 3.3220 - val_acc: 0.6194\n",
      "Epoch 118/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1305 - acc: 0.9384 - val_loss: 3.1869 - val_acc: 0.6012\n",
      "Epoch 119/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1405 - acc: 0.9379 - val_loss: 3.2941 - val_acc: 0.6083\n",
      "Epoch 120/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1357 - acc: 0.9402 - val_loss: 3.3953 - val_acc: 0.5941\n",
      "Epoch 121/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1323 - acc: 0.9384 - val_loss: 3.4879 - val_acc: 0.5891\n",
      "Epoch 122/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1307 - acc: 0.9443 - val_loss: 3.3553 - val_acc: 0.5901\n",
      "Epoch 123/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1398 - acc: 0.9400 - val_loss: 3.3694 - val_acc: 0.5931\n",
      "Epoch 124/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1297 - acc: 0.9420 - val_loss: 3.2920 - val_acc: 0.5931\n",
      "Epoch 125/125\n",
      "3948/3948 [==============================] - 4s - loss: 0.1295 - acc: 0.9425 - val_loss: 3.4523 - val_acc: 0.5982\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_text, Y, \n",
    "                 batch_size=100, nb_epoch=125, verbose=1, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 500, 300)          821100    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 500, 512)          1665024   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 3,407,216\n",
      "Trainable params: 3,407,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model1 Built\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(2737, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Embedding(nb_words,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights = [g_word_embedding_matrix],\n",
    "                    input_length = MAX_SEQUENCE_LENGTH,\n",
    "                    trainable = True))\n",
    "\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(LSTM(256, return_sequences=False))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "#sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam' ,metrics=['acc'])\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "#model.compile()\n",
    "model.summary()\n",
    "\n",
    "print(\"Model1 Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/30\n",
      "3948/3948 [==============================] - 79s - loss: 1.3064 - acc: 0.3934 - val_loss: 1.1940 - val_acc: 0.4808\n",
      "Epoch 2/30\n",
      "3948/3948 [==============================] - 80s - loss: 1.0557 - acc: 0.5537 - val_loss: 0.9880 - val_acc: 0.5941\n",
      "Epoch 3/30\n",
      "3948/3948 [==============================] - 80s - loss: 0.8372 - acc: 0.6593 - val_loss: 0.9241 - val_acc: 0.6397\n",
      "Epoch 4/30\n",
      "3948/3948 [==============================] - 80s - loss: 0.6876 - acc: 0.7305 - val_loss: 0.9942 - val_acc: 0.6063\n",
      "Epoch 5/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.5866 - acc: 0.7687 - val_loss: 1.0076 - val_acc: 0.6245\n",
      "Epoch 6/30\n",
      "3948/3948 [==============================] - 80s - loss: 0.4834 - acc: 0.8116 - val_loss: 1.1072 - val_acc: 0.6336\n",
      "Epoch 7/30\n",
      "3948/3948 [==============================] - 80s - loss: 0.4247 - acc: 0.8293 - val_loss: 1.1809 - val_acc: 0.6306\n",
      "Epoch 8/30\n",
      "3948/3948 [==============================] - 80s - loss: 0.3744 - acc: 0.8478 - val_loss: 1.3776 - val_acc: 0.6103\n",
      "Epoch 9/30\n",
      "3948/3948 [==============================] - 80s - loss: 0.3723 - acc: 0.8531 - val_loss: 1.2070 - val_acc: 0.6356\n",
      "Epoch 10/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.2980 - acc: 0.8769 - val_loss: 1.3819 - val_acc: 0.6306\n",
      "Epoch 11/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.2567 - acc: 0.8977 - val_loss: 1.4406 - val_acc: 0.6306\n",
      "Epoch 12/30\n",
      "3948/3948 [==============================] - 80s - loss: 0.2437 - acc: 0.9020 - val_loss: 1.6528 - val_acc: 0.6437\n",
      "Epoch 13/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.2172 - acc: 0.9119 - val_loss: 1.6849 - val_acc: 0.6285\n",
      "Epoch 14/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.2017 - acc: 0.9149 - val_loss: 2.0301 - val_acc: 0.6093\n",
      "Epoch 15/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.2071 - acc: 0.9106 - val_loss: 1.8266 - val_acc: 0.6326\n",
      "Epoch 16/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.1906 - acc: 0.9210 - val_loss: 1.9271 - val_acc: 0.6275\n",
      "Epoch 17/30\n",
      "3948/3948 [==============================] - 80s - loss: 0.4561 - acc: 0.8518 - val_loss: 1.2880 - val_acc: 0.5547\n",
      "Epoch 18/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.5967 - acc: 0.7832 - val_loss: 1.5388 - val_acc: 0.4494\n",
      "Epoch 19/30\n",
      "3948/3948 [==============================] - 79s - loss: 1.1812 - acc: 0.4810 - val_loss: 1.1062 - val_acc: 0.5526\n",
      "Epoch 20/30\n",
      "3948/3948 [==============================] - 80s - loss: 0.4784 - acc: 0.8108 - val_loss: 1.2308 - val_acc: 0.5992\n",
      "Epoch 21/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.3337 - acc: 0.8658 - val_loss: 1.2986 - val_acc: 0.6336\n",
      "Epoch 22/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.2750 - acc: 0.8916 - val_loss: 1.4909 - val_acc: 0.6285\n",
      "Epoch 23/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.2464 - acc: 0.8999 - val_loss: 1.7960 - val_acc: 0.5860\n",
      "Epoch 24/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.2196 - acc: 0.9083 - val_loss: 1.6291 - val_acc: 0.6215\n",
      "Epoch 25/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.2031 - acc: 0.9202 - val_loss: 1.6109 - val_acc: 0.6377\n",
      "Epoch 26/30\n",
      "3948/3948 [==============================] - 80s - loss: 0.1837 - acc: 0.9210 - val_loss: 1.8977 - val_acc: 0.6468\n",
      "Epoch 27/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.1748 - acc: 0.9260 - val_loss: 1.9842 - val_acc: 0.6194\n",
      "Epoch 28/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.1636 - acc: 0.9283 - val_loss: 2.0618 - val_acc: 0.6468\n",
      "Epoch 29/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.1561 - acc: 0.9324 - val_loss: 2.1641 - val_acc: 0.6204\n",
      "Epoch 30/30\n",
      "3948/3948 [==============================] - 79s - loss: 0.1575 - acc: 0.9293 - val_loss: 2.1435 - val_acc: 0.6204\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_text, Y, \n",
    "                 batch_size=100, nb_epoch=30, verbose=1, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2736"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 128)         350208    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 256)         394240    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,403,396\n",
      "Trainable params: 1,403,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model1 Built\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(Embedding(2737, 128, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Embedding(2736,\n",
    "                    128))\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256, return_sequences=False))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "#sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam' ,metrics=['acc'])\n",
    "\n",
    "## compille it here according to instructions\n",
    "\n",
    "#model.compile()\n",
    "model.summary()\n",
    "\n",
    "print(\"Model1 Built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3948 samples, validate on 988 samples\n",
      "Epoch 1/30\n",
      "3948/3948 [==============================] - 65s - loss: 1.3567 - acc: 0.3478 - val_loss: 1.2947 - val_acc: 0.4302\n",
      "Epoch 2/30\n",
      "3948/3948 [==============================] - 64s - loss: 1.0827 - acc: 0.5466 - val_loss: 1.0113 - val_acc: 0.5850\n",
      "Epoch 3/30\n",
      "3948/3948 [==============================] - 63s - loss: 0.7808 - acc: 0.6920 - val_loss: 1.0052 - val_acc: 0.5992\n",
      "Epoch 4/30\n",
      "3948/3948 [==============================] - 62s - loss: 0.6202 - acc: 0.7604 - val_loss: 0.9847 - val_acc: 0.6478\n",
      "Epoch 5/30\n",
      "3948/3948 [==============================] - 62s - loss: 0.5411 - acc: 0.7943 - val_loss: 1.0931 - val_acc: 0.6407\n",
      "Epoch 6/30\n",
      "3948/3948 [==============================] - 62s - loss: 0.4679 - acc: 0.8207 - val_loss: 1.1445 - val_acc: 0.6134\n",
      "Epoch 7/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.4165 - acc: 0.8425 - val_loss: 1.2533 - val_acc: 0.6174\n",
      "Epoch 8/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.4249 - acc: 0.8323 - val_loss: 1.3206 - val_acc: 0.6245\n",
      "Epoch 9/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.3900 - acc: 0.8447 - val_loss: 1.3418 - val_acc: 0.6204\n",
      "Epoch 10/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.3492 - acc: 0.8609 - val_loss: 1.3772 - val_acc: 0.6063\n",
      "Epoch 11/30\n",
      "3948/3948 [==============================] - 65s - loss: 0.3275 - acc: 0.8756 - val_loss: 1.5439 - val_acc: 0.6053\n",
      "Epoch 12/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.3094 - acc: 0.8784 - val_loss: 1.5729 - val_acc: 0.6022\n",
      "Epoch 13/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.2851 - acc: 0.8880 - val_loss: 1.7941 - val_acc: 0.5931\n",
      "Epoch 14/30\n",
      "3948/3948 [==============================] - 63s - loss: 0.2847 - acc: 0.8825 - val_loss: 1.7680 - val_acc: 0.6144\n",
      "Epoch 15/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.2990 - acc: 0.8853 - val_loss: 1.8876 - val_acc: 0.5901\n",
      "Epoch 16/30\n",
      "3948/3948 [==============================] - 65s - loss: 0.2921 - acc: 0.8840 - val_loss: 1.6733 - val_acc: 0.5820\n",
      "Epoch 17/30\n",
      "3948/3948 [==============================] - 65s - loss: 0.2844 - acc: 0.8855 - val_loss: 1.6550 - val_acc: 0.6134\n",
      "Epoch 18/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.2406 - acc: 0.9002 - val_loss: 1.8191 - val_acc: 0.6032\n",
      "Epoch 19/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.2222 - acc: 0.9116 - val_loss: 1.9185 - val_acc: 0.5911\n",
      "Epoch 20/30\n",
      "3948/3948 [==============================] - 63s - loss: 0.2103 - acc: 0.9106 - val_loss: 2.0810 - val_acc: 0.6134\n",
      "Epoch 21/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.2147 - acc: 0.9088 - val_loss: 2.1386 - val_acc: 0.5982\n",
      "Epoch 22/30\n",
      "3948/3948 [==============================] - 65s - loss: 0.2014 - acc: 0.9124 - val_loss: 2.2030 - val_acc: 0.5941\n",
      "Epoch 23/30\n",
      "3948/3948 [==============================] - 65s - loss: 0.1908 - acc: 0.9210 - val_loss: 2.3497 - val_acc: 0.5830\n",
      "Epoch 24/30\n",
      "3948/3948 [==============================] - 65s - loss: 0.1904 - acc: 0.9187 - val_loss: 2.2329 - val_acc: 0.5982\n",
      "Epoch 25/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.1854 - acc: 0.9189 - val_loss: 2.3072 - val_acc: 0.5891\n",
      "Epoch 26/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.1996 - acc: 0.9189 - val_loss: 2.3266 - val_acc: 0.5931\n",
      "Epoch 27/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.2306 - acc: 0.9136 - val_loss: 2.1436 - val_acc: 0.6043\n",
      "Epoch 28/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.1941 - acc: 0.9222 - val_loss: 2.2942 - val_acc: 0.5891\n",
      "Epoch 29/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.1789 - acc: 0.9205 - val_loss: 2.3057 - val_acc: 0.5931\n",
      "Epoch 30/30\n",
      "3948/3948 [==============================] - 64s - loss: 0.1702 - acc: 0.9260 - val_loss: 2.3577 - val_acc: 0.6093\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x_train_text, Y, \n",
    "                 batch_size=100, nb_epoch=30, verbose=1, \n",
    "                 validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
